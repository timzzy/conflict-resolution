{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "canadian-diploma",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import the functions file\n",
    "import useful_functions\n",
    "# import coreNLP_function zzz\n",
    "\n",
    "\n",
    "\n",
    "# import other python useful modules\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "from sklearn import tree\n",
    "import graphviz\n",
    "\n",
    "# elmo stuff\n",
    "import tensorflow_hub as hub\n",
    "import tensorflow.compat.v1 as tf\n",
    "tf.disable_eager_execution()\n",
    "\n",
    "from simple_elmo import ElmoModel\n",
    "\n",
    "\n",
    "from sklearn import metrics\n",
    "from sklearn.model_selection import GridSearchCV, cross_val_score, cross_val_predict, train_test_split\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "from functools import reduce\n",
    "import pickle\n",
    "import random\n",
    "\n",
    "\n",
    "# some NLP modules\n",
    "import spacy\n",
    "from tqdm import tqdm\n",
    "import re\n",
    "import time\n",
    "\n",
    "pd.set_option('display.max_colwidth', 200)\n",
    "import nltk\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import sent_tokenize, word_tokenize\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "import unidecode\n",
    "import contractions\n",
    "import re\n",
    "from textblob import Word\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "backed-chess",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required package for scraping android reviews\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pygments import highlight\n",
    "from pygments.lexers import JsonLexer\n",
    "from pygments.formatters import TerminalFormatter\n",
    "\n",
    "from google_play_scraper import Sort, reviews, app\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format='retina'\n",
    "\n",
    "sns.set(style='whitegrid', palette='muted', font_scale=1.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-blank",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required package for Mining Apple App's Review\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "\n",
    "driver = webdriver.Chrome()\n",
    "driver.get('https://sensortower.com/ios/US/twelve-app/app/yubo-make-new-friends/1038653883/review-history?selected_tab=reviews')\n",
    "\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import Stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "studied-conflict",
   "metadata": {},
   "outputs": [],
   "source": [
    "# zzz\n",
    "# module = hub.Module(\"https://tfhub.dev/google/universal-sentence-encoder-large/3\")\n",
    "# el_model = ElmoModel()\n",
    "# el_model.load(\"\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "activated-johnson",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Some reqquired module to be installed\n",
    "# !pip install -qq google-play-scraper\n",
    "# !pip install -qq -U watermark\n",
    "# !pip install google-play-scraper\n",
    "# !pip install contractions\n",
    "# !pip install spacy\n",
    "# !pip install textblob"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "administrative-ontario",
   "metadata": {},
   "source": [
    "### - Starts Scrape Android App Review"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "soviet-drinking",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the app packages for some android applications whose reviews we want to scrap...\n",
    "# More android packages for diffrent android apps can be added depending on the quantity of reviewsw we want to extract\n",
    "\n",
    "app_packages = [\n",
    "  'com.anydo',\n",
    "  'com.todoist',\n",
    "  'com.ticktick.task',\n",
    "  'com.habitrpg.android.habitica',\n",
    "  'cc.forestapp',\n",
    "  'com.oristats.habitbull',\n",
    "  'com.levor.liferpgtasks',\n",
    "  'com.habitnow',\n",
    "  'com.microsoft.todos',\n",
    "  'prox.lab.calclock',\n",
    "  'com.gmail.jmartindev.timetune',\n",
    "  'com.artfulagenda.app',\n",
    "  'com.tasks.android',\n",
    "  'com.appgenix.bizcal',\n",
    "  'com.appxy.planner'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "terminal-cartridge",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets loop through the apps an get their info\n",
    "app_infos = []\n",
    "\n",
    "for ap in tqdm(app_packages):\n",
    "  info = app(ap, lang='en', country='us')\n",
    "  del info['comments']\n",
    "  app_infos.append(info)\n",
    "\n",
    "# This is a function that prints the app's info\n",
    "def print_json(json_object):\n",
    "  json_str = json.dumps(\n",
    "    json_object, \n",
    "    indent=2, \n",
    "    sort_keys=True, \n",
    "    default=str\n",
    "  )\n",
    "  print(highlight(json_str, JsonLexer(), TerminalFormatter()))\n",
    "\n",
    "    \n",
    "# This contains lots of information including the number of ratings, number of reviews and number of ratings for each score (1 to 5). \n",
    "\n",
    "# Lets test the print_json function\n",
    "# print_json(app_infos[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-insert",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scraping the reviews\n",
    "app_reviews = []\n",
    "\n",
    "for ap in tqdm(app_packages):\n",
    "  for score in list(range(1, 6)):\n",
    "    for sort_order in [Sort.MOST_RELEVANT, Sort.NEWEST]:\n",
    "      rvs, _ = reviews(\n",
    "        ap,\n",
    "        lang='en',\n",
    "        country='us',\n",
    "        sort=sort_order,\n",
    "        count= 200 if score == 3 else 100,\n",
    "        filter_score_with=score\n",
    "      )\n",
    "      for r in rvs:\n",
    "        r['sortOrder'] = 'most_relevant' if sort_order == Sort.MOST_RELEVANT else 'newest'\n",
    "        r['appId'] = ap\n",
    "      app_reviews.extend(rvs)\n",
    "    \n",
    "\n",
    "    \n",
    "# Lets get the length of app reviews\n",
    "# len(app_reviews)\n",
    "\n",
    "# Now, we save the review dataframes to a csv file which will be re-imported latter for further processing\n",
    "app_reviews_df = pd.DataFrame(app_reviews)\n",
    "app_reviews_df.to_csv('datasets/android_reviews.csv', index=None, header=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "endangered-notification",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "obvious-section",
   "metadata": {},
   "source": [
    "### - Starts Scrape Apple App Review\n",
    " The reviews section for apple product  doesn't have a download button, so we use a Selenium web scraper to download the information instead. The packages have been imported in the beginning section of this file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-synthesis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Required Installs\n",
    "#!pip install pystemmer\n",
    "#!pip install sklearn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "exclusive-annotation",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function\n",
    "\n",
    "def get_page():\n",
    "    doc = BeautifulSoup(driver.page_source)\n",
    "    rows = doc.select(\"tbody tr\")\n",
    "\n",
    "    datapoints = []\n",
    "    for row in rows:\n",
    "        cells = row.select(\"td\")\n",
    "        data = {\n",
    "            'Country': cells[0].text.strip(),\n",
    "            'Date': cells[1].text.strip(),\n",
    "            'Rating': cells[2].select_one('.gold')['style'],\n",
    "            'Review': cells[3].select_one('.break-wrap-review').text.strip(),\n",
    "            'Version': cells[4].text.strip()\n",
    "        }\n",
    "        datapoints.append(data)\n",
    "    return datapoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "valid-trader",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_data = []\n",
    "wait = WebDriverWait(driver, 5, poll_frequency=0.05)\n",
    "while True:\n",
    "    wait.until(EC.invisibility_of_element_located((By.CSS_SELECTOR, '.ajax-loading-cover')))\n",
    "\n",
    "    results = get_page()    \n",
    "    all_data.extend(results)\n",
    "\n",
    "    next_button = driver.find_elements_by_css_selector(\".btn-group .pagination\")[1]\n",
    "    if next_button.get_attribute('disabled'):\n",
    "        break\n",
    "    next_button.click()\n",
    "    time.sleep(0.5)\n",
    "    # Doesn't trigger fast enough!\n",
    "    # wait.until(EC.visibility_of_element_located((By.CSS_SELECTOR, '.ajax-loading-cover')))\n",
    "  \n",
    "   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "average-romania",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lets save the reviews into a csv file\n",
    "df_apple = pd.DataFrame(all_data)\n",
    "df_apple.to_csv(\"datasets/apple_reviews.csv\", index=None, header=True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "functioning-receipt",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TExt Vectorization\n",
    "# English stemmer from pyStemmer\n",
    "stemmer = Stemmer.Stemmer('en')\n",
    "\n",
    "analyzer = TfidfVectorizer().build_analyzer()\n",
    "\n",
    "# Override CountVectorizer\n",
    "class StemmedTfidfVectorizer(TfidfVectorizer):\n",
    "    def build_analyzer(self):\n",
    "        analyzer = super(TfidfVectorizer, self).build_analyzer()\n",
    "        return lambda doc: stemmer.stemWords(analyzer(doc))\n",
    "\n",
    "# Create a new StemmedCountVectorizer\n",
    "vectorizer = StemmedTfidfVectorizer()\n",
    "matrix = vectorizer.fit_transform(known.Review)\n",
    "\n",
    "# Build a dataframe of words, purely out of curiosity\n",
    "words_df = pd.DataFrame(matrix.toarray(), columns=vectorizer.get_feature_names())\n",
    "words_df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "falling-domain",
   "metadata": {},
   "source": [
    "### Lets import the kaggle, android and apple datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "later-chest",
   "metadata": {},
   "outputs": [],
   "source": [
    "android_df = pd.read_csv(\"datasets/android_reviews.csv\")\n",
    "android_df = android_df.dropna(subset=['Review'])\n",
    "android_df = android_df.drop(columns=['reviewID', 'userName', 'userImage', 'score', 'replyContent'])\n",
    "\n",
    "apple_df = pd.read_csv(\"datasets/apple_reviews.csv\")\n",
    "apple_df = apple_df.dropna(subset=['Review'])\n",
    "apple_df = apple_df.drop(columns=['Ratings', 'Comments'])\n",
    "\n",
    "\n",
    "kaggle_df = pd.read_csv(\"datasets/kaggle_reviews.csv\")\n",
    "kaggle_df = kaggle_df.dropna(subset=['Review'])\n",
    "kaggle_df = kaggle_df.drop(columns=['App', 'Sentiment', 'Sentiment_polarity', 'Sentiment_Subjectivity', ''])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "technological-correspondence",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reviews \n",
    "android_review = android_df['Reviews']\n",
    "apple_review = apple_df['Reviews']\n",
    "kaggle_review = kaggle_df['Reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fossil-motion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# part of speech tagging\n",
    "android_words = word_tokenize(android_df['Reviews'])\n",
    "apple_words = word_tokenize(apple_df['Reviews'])\n",
    "kaggle_words = word_tokenize(kaggle_df['Reviews'])\n",
    "\n",
    "android_pos_tag = nltk.pos_tag(android_words)\n",
    "apple_pos_tag = nltk.pos_tag(apple_words)\n",
    "kaggle_pos_tag = nltk.pos_tag(kaggle_words)\n",
    "\n",
    "# Removal of accented Characters\n",
    "unaccented_string_android = unidecode.unidecode(android_df['Reviews'])\n",
    "unaccented_string_apple = unidecode.unidecode(apple_df['Reviews'])\n",
    "unaccented_string_kaggle = unidecode.unidecode(kaggle_df['Reviews'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "plain-arrival",
   "metadata": {},
   "outputs": [],
   "source": [
    "porter_stemmer = PorterStemmer()\n",
    "word_lemmatizer = WordNetLemmatizer()\n",
    "\n",
    "\n",
    "stem_android_review = porter_stemmer.stem(android_df['Reviews'])\n",
    "stem_apple_review = porter_stemmer.stem(apple_df['Reviews'])\n",
    "stem_kaggle_review = porter_stemmer.stem(kaggle_df['Reviews'])\n",
    "\n",
    "\n",
    "lemmatize_android_review = porter_stemmer.stem(android_df['Reviews'])\n",
    "lemmatize_apple_review = porter_stemmer.stem(apple_df['Reviews'])\n",
    "lemmatize_kaggle_review = porter_stemmer.stem(kaggle_df['Reviews'])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "geological-theorem",
   "metadata": {},
   "outputs": [],
   "source": [
    "#  print samples of the word stemming and lemmatizing\n",
    "for w in word_tokenize(android_df['Reviews']):\n",
    "#     print(f'Actual Word - {w}')\n",
    "#     print(f'Stem - {porter_stemmer.stem(w)}')\n",
    "#     print(f'Lemma - {word_lemmatizer.lemmatize(w)}\\n')\n",
    "\n",
    "for q in word_tokenize(apple_df['Reviews']):\n",
    "#     print(f'Actual Word - {q}')\n",
    "#     print(f'Stem - {porter_stemmer.stem(q)}')\n",
    "#     print(f'Lemma - {word_lemmatizer.lemmatize(q)}\\n')\n",
    "\n",
    "for t in word_tokenize(kaggle_df['Reviews']):\n",
    "#     print(f'Actual Word - {t}')\n",
    "#     print(f'Stem - {porter_stemmer.stem(t)}')\n",
    "#     print(f'Lemma - {word_lemmatizer.lemmatize(t)}\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dental-arthritis",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Lowercasing\n",
    "android_df['Reviews'] = android_df['Reviews'].str.lower()\n",
    "apple_df['Reviews'] = apple_df['Reviews'].str.lower()\n",
    "kaggle_df['Reviews'] = kaggle_df['Reviews'].str.lower()\n",
    "\n",
    "\n",
    "# Remove special character\n",
    "android_df['Reviews'] = re.sub(r\"[^a-zA-Z0-9]\",\" \",android_df['Reviews'])\n",
    "apple_df['Reviews'] = re.sub(r\"[^a-zA-Z0-9]\",\" \",apple_df['Reviews'])\n",
    "kaggle_df['Reviews'] = re.sub(r\"[^a-zA-Z0-9]\",\" \",kaggle_df['Reviews'])\n",
    "\n",
    "\n",
    "# Expanding  contractions\n",
    "android_expanded_words = []   \n",
    "apple_expanded_words = []   \n",
    "kaggle_expanded_words = []   \n",
    "\n",
    "for word in android_df['Reviews'].split():\n",
    "  # using contractions.fix to expand the shortened words\n",
    "  expanded_words.append(contractions.fix(word))  \n",
    "   \n",
    "expanded_text_android = ' '.join(expanded_words_android)\n",
    "\n",
    "for word in apple_df['Reviews'].split():\n",
    "  # using contractions.fix to expand the shortened words\n",
    "  expanded_words.append(contractions.fix(word))  \n",
    "   \n",
    "expanded_text_apple = ' '.join(expanded_words_apple)\n",
    "\n",
    "for word in kaggle_df['Reviews'].split():\n",
    "  # using contractions.fix to expand the shortened words\n",
    "  expanded_words.append(contractions.fix(word))  \n",
    "   \n",
    "expanded_text_kaggle = ' '.join(expanded_words_kaggle)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "divine-leonard",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to remove stopword\n",
    "all_stopwords = sp.Defaults.stop_words\n",
    "\n",
    "def remove_stopwords(sentence):\n",
    "    tokens = sentence.split(\" \")\n",
    "    tokens_filtered= [word for word in text_tokens if not word in all_stopwords]\n",
    "    return (\" \").join(tokens_filtered)\n",
    "\n",
    "android_tokens = remove_stopwords(android_df['Reviews'])\n",
    "apple_tokens = remove_stopwords(apple_df['Reviews'])\n",
    "kaggle_tokens = remove_stopwords(kaggle_df['Reviews'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "necessary-crystal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# correct mispelled words\n",
    "def correct_word_spelling(word):\n",
    "    word = Word(word)\n",
    "    result = word.correct()\n",
    "    return(result)\n",
    "\n",
    "corrected_android_review = android_df['Reviews']\n",
    "corrected_apple_review = apple_df['Reviews']\n",
    "corrected_kaggle_review = kaggle_df['Reviews']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "amber-handle",
   "metadata": {},
   "outputs": [],
   "source": [
    "# here we combine the review columns for each dataframe and \n",
    "df['combined_review'] = df.apply(lambda row: ' '.join([str(android_df['Reviews']), \n",
    "                                            str(apple_df['Reviews']), \n",
    "                                            str(kaggle_df['Reviews'])]), axis=1)\n",
    "\n",
    "# annotate the extracted features from text\n",
    "df['char_count'] = df['combined_review'].apply(len)\n",
    "df['word_count'] = df['combined_review'].apply(lambda x: len(x.split()))\n",
    "df['word_density'] = df['char_count'] / (df['word_count']+1)\n",
    "df['punctuation_count'] = df['combined_review'].apply(lambda x: len(\"\".join(_ for _ in x if _ in punctuation))) \n",
    "df['title_word_count'] = df['combined_review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.istitle()]))\n",
    "df['upper_case_word_count'] = df['combined_review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.isupper()]))\n",
    "df['stopword_count'] = df['combined_review'].apply(lambda x: len([wrd for wrd in x.split() if wrd.lower() in stop_words]))\n",
    "\n",
    "# display the overview\n",
    "# df[['char_count', 'word_count', 'word_density', 'punctuation_count', 'title_word_count', 'upper_case_word_count', 'stopword_count']].head(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "refined-garbage",
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to generate sentence vector of the sentence\n",
    "\n",
    "def vectorize_review(sentence):\n",
    "    M = []\n",
    "    for w in word_tokenize(sentence):\n",
    "#     for w in word_tokenize(unicode(sentence, 'utf8')):\n",
    "        if not w.isalpha():\n",
    "            continue\n",
    "        if w in embeddings_index:\n",
    "            M.append(embeddings_index[w])\n",
    "    M = np.array(M)\n",
    "    v = M.sum(axis=0)\n",
    "    if type(v) != np.ndarray:\n",
    "        return np.zeros(300)\n",
    "    return v / np.sqrt((v ** 2).sum())\n",
    "\n",
    "xtrain_vector = [sent2vec(x) for x in xtrain[:10]]\n",
    "xtrain_vector = np.array(xtrain_vector)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "impaired-senate",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create count vectorizer function to do sample topic modeling first\n",
    "cvectorizer = CountVectorizer(min_df=4, max_features=4000, ngram_range=(1,2))\n",
    "cvz = cvectorizer.fit_transform(df['text'])\n",
    "\n",
    "# generate topic models using Latent Dirichlet Allocation\n",
    "lda_model = LatentDirichletAllocation(n_components=20, learning_method='online', max_iter=20, random_state=42)\n",
    "X_topics = lda_model.fit_transform(cvz)\n",
    "n_top_words = 10\n",
    "topic_summaries = []\n",
    "\n",
    "# get topics and topic terms\n",
    "topic_word = lda_model.components_ \n",
    "vocab = cvectorizer.get_feature_names()\n",
    "\n",
    "for i, topic_dist in enumerate(topic_word):\n",
    "    topic_words = np.array(vocab)[np.argsort(topic_dist)][:-(n_top_words+1):-1]\n",
    "    topic_summaries.append(' '.join(topic_words))\n",
    "#     print('Topic {}: {}'.format(i, ' | '.join(topic_words)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "final-sphere",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "robust-motivation",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "controlled-fence",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "prime-archive",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gpu_lstm_tensorflow",
   "language": "python",
   "name": "gpu_lstm_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
